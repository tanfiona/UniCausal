{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "506a81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "verbose = False\n",
    "# To get this pickle file: Run \"_Draft\" first half\n",
    "version = 'v1.0' #'v0.9'\n",
    "with open(f\"esl_document_raw_{version}.pickle\", 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "    \n",
    "list_loc = {\n",
    "    'all_token':0, \n",
    "    'ecb_star_events':1, \n",
    "    'ecb_coref_relations':2, \n",
    "    'ecb_star_time':3, \n",
    "    'ecbstar_events_plotLink':4, \n",
    "    'ecbstar_timelink':5, \n",
    "    'evaluation_data':6, \n",
    "    'evaluationcrof_data':7\n",
    "}\n",
    "\n",
    "token_loc = {\n",
    "    'global_loc':0,\n",
    "    'sent_id':1,\n",
    "    'word_id':2,\n",
    "    'word':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f10c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5188\n",
      "2279 3854\n",
      "4637 4519\n",
      "5684 2279\n"
     ]
    }
   ],
   "source": [
    "num_of_events = 0\n",
    "num_plotlinks = 0\n",
    "num_rising = 0\n",
    "num_falling = 0\n",
    "num_tlinks = 0\n",
    "num_rel = 0\n",
    "num_corefrel = 0\n",
    "\n",
    "for k,v in data_dict.items():\n",
    "    num_of_events+=len(v[list_loc['ecb_star_events']])\n",
    "    num_plotlinks += len(v[list_loc['ecbstar_events_plotLink']])\n",
    "    num_tlinks += len(v[list_loc['ecbstar_timelink']])\n",
    "    num_rel += len(v[list_loc['evaluation_data']])\n",
    "    num_corefrel += len(v[list_loc['evaluationcrof_data']])\n",
    "    \n",
    "    \n",
    "    for h,t,label in v[list_loc['evaluationcrof_data']]:\n",
    "        count = len(h.split(' '))*len(t.split(' '))    \n",
    "        if label=='PRECONDITION':\n",
    "            num_rising+=count\n",
    "        elif label=='FALLING_ACTION':\n",
    "            num_falling+=count\n",
    "\n",
    "print(num_of_events)\n",
    "print(num_plotlinks, num_tlinks)\n",
    "print(num_falling, num_rising)\n",
    "print(num_rel, num_corefrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d17eb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['52', '57', '62', '70', '72', '124', '79', '126', '132', '170', '175_176_177_178', '186_187_188_189', '194', '197', '201', '40', '39', '41', '191', '55', '36'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[list_loc['ecb_star_events']].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e366d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('179', '57'): 'CONTAINS',\n",
       "             ('179', '62'): 'CONTAINS',\n",
       "             ('179', '70'): 'CONTAINS',\n",
       "             ('179', '72'): 'CONTAINS',\n",
       "             ('179', '124'): 'CONTAINS',\n",
       "             ('179', '126'): 'CONTAINS',\n",
       "             ('179', '132'): 'CONTAINS',\n",
       "             ('179', '170'): 'CONTAINS',\n",
       "             ('179', '175_176_177_178'): 'CONTAINS',\n",
       "             ('179', '186_187_188_189'): 'CONTAINS',\n",
       "             ('179', '191'): 'CONTAINS',\n",
       "             ('43_44_45_46', '57'): 'CONTAINS',\n",
       "             ('43_44_45_46', '62'): 'CONTAINS',\n",
       "             ('43_44_45_46', '70'): 'CONTAINS',\n",
       "             ('43_44_45_46', '72'): 'CONTAINS',\n",
       "             ('43_44_45_46', '79'): 'BEFORE',\n",
       "             ('43_44_45_46', '124'): 'CONTAINS',\n",
       "             ('43_44_45_46', '126'): 'CONTAINS',\n",
       "             ('43_44_45_46', '132'): 'AFTER',\n",
       "             ('43_44_45_46', '170'): 'CONTAINS',\n",
       "             ('43_44_45_46', '175_176_177_178'): 'CONTAINS',\n",
       "             ('43_44_45_46', '186_187_188_189'): 'CONTAINS',\n",
       "             ('43_44_45_46', '191'): 'CONTAINS',\n",
       "             ('43_44_45_46', '194'): 'CONTAINS',\n",
       "             ('43_44_45_46', '197'): 'BEFORE',\n",
       "             ('43_44_45_46', '201'): 'AFTER',\n",
       "             ('43_44_45_46', '39'): 'CONTAINS',\n",
       "             ('43_44_45_46', '40'): 'CONTAINS',\n",
       "             ('43_44_45_46', '41'): 'CONTAINS',\n",
       "             ('179', '39'): 'CONTAINS',\n",
       "             ('179', '40'): 'CONTAINS',\n",
       "             ('179', '41'): 'CONTAINS'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[list_loc['ecbstar_timelink']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7de7ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['39', '57', 'PRECONDITION'],\n",
       " ['39', '79', 'FALLING_ACTION'],\n",
       " ['39', '186_187_188_189', 'PRECONDITION'],\n",
       " ['41', '70', 'FALLING_ACTION'],\n",
       " ['39', '194', 'FALLING_ACTION'],\n",
       " ['124', '170', 'PRECONDITION'],\n",
       " ['57', '191', 'FALLING_ACTION'],\n",
       " ['57', '62', 'FALLING_ACTION'],\n",
       " ['40', '191', 'FALLING_ACTION'],\n",
       " ['72', '186_187_188_189', 'PRECONDITION'],\n",
       " ['62', '126', 'FALLING_ACTION'],\n",
       " ['40', '41', 'PRECONDITION'],\n",
       " ['57', '124', 'FALLING_ACTION'],\n",
       " ['62', '72', 'FALLING_ACTION'],\n",
       " ['191', '194', 'FALLING_ACTION'],\n",
       " ['70', '170', 'PRECONDITION'],\n",
       " ['57', '170', 'FALLING_ACTION'],\n",
       " ['72', '124', 'PRECONDITION'],\n",
       " ['39', '40', 'PRECONDITION'],\n",
       " ['62', '191', 'FALLING_ACTION'],\n",
       " ['39', '170', 'PRECONDITION'],\n",
       " ['40', '70', 'FALLING_ACTION'],\n",
       " ['57', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['41', '170', 'FALLING_ACTION'],\n",
       " ['40', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['170', '191', 'FALLING_ACTION'],\n",
       " ['57', '72', 'FALLING_ACTION'],\n",
       " ['70', '72', 'FALLING_ACTION'],\n",
       " ['126', '186_187_188_189', 'PRECONDITION'],\n",
       " ['62', '170', 'PRECONDITION'],\n",
       " ['40', '124', 'PRECONDITION'],\n",
       " ['70', '126', 'FALLING_ACTION'],\n",
       " ['39', '132', 'PRECONDITION'],\n",
       " ['79', '191', 'PRECONDITION'],\n",
       " ['170', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['40', '57', 'FALLING_ACTION'],\n",
       " ['40', '170', 'PRECONDITION'],\n",
       " ['124', '126', 'FALLING_ACTION'],\n",
       " ['40', '62', 'PRECONDITION'],\n",
       " ['41', '57', 'FALLING_ACTION'],\n",
       " ['70', '124', 'PRECONDITION'],\n",
       " ['72', '194', 'FALLING_ACTION'],\n",
       " ['126', '194', 'FALLING_ACTION'],\n",
       " ['40', '72', 'FALLING_ACTION'],\n",
       " ['72', '132', 'PRECONDITION'],\n",
       " ['70', '191', 'FALLING_ACTION'],\n",
       " ['57', '126', 'FALLING_ACTION'],\n",
       " ['126', '170', 'PRECONDITION'],\n",
       " ['126', '132', 'PRECONDITION'],\n",
       " ['62', '70', 'FALLING_ACTION'],\n",
       " ['194', '197', 'PRECONDITION'],\n",
       " ['194', '201', 'PRECONDITION'],\n",
       " ['124', '191', 'FALLING_ACTION'],\n",
       " ['72', '79', 'FALLING_ACTION'],\n",
       " ['186_187_188_189', '191', 'FALLING_ACTION'],\n",
       " ['39', '62', 'PRECONDITION'],\n",
       " ['41', '124', 'FALLING_ACTION'],\n",
       " ['41', '62', 'FALLING_ACTION'],\n",
       " ['39', '124', 'PRECONDITION'],\n",
       " ['79', '126', 'PRECONDITION'],\n",
       " ['72', '170', 'PRECONDITION'],\n",
       " ['39', '70', 'PRECONDITION'],\n",
       " ['124', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['132', '191', 'FALLING_ACTION'],\n",
       " ['57', '70', 'FALLING_ACTION'],\n",
       " ['62', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['62', '124', 'FALLING_ACTION'],\n",
       " ['40', '126', 'FALLING_ACTION']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[list_loc['evaluation_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd71f9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['57 170 40 62 124', '70', 'FALLING_ACTION'],\n",
       " ['194', '197', 'PRECONDITION'],\n",
       " ['57 170 40 62 124', '175_176_177_178', 'FALLING_ACTION'],\n",
       " ['41', '70', 'FALLING_ACTION'],\n",
       " ['191 39 126 72', '132', 'PRECONDITION'],\n",
       " ['194', '201', 'PRECONDITION'],\n",
       " ['57 170 40 62 124', '57 170 40 62 124', 'FALLING_ACTION'],\n",
       " ['191 39 126 72', '79', 'FALLING_ACTION'],\n",
       " ['70', '191 39 126 72', 'FALLING_ACTION'],\n",
       " ['41', '57 170 40 62 124', 'FALLING_ACTION'],\n",
       " ['57 170 40 62 124', '70', 'FALLING_ACTION'],\n",
       " ['186_187_188_189', '191 39 126 72', 'FALLING_ACTION'],\n",
       " ['191 39 126 72', '57 170 40 62 124', 'PRECONDITION'],\n",
       " ['191 39 126 72', '194', 'FALLING_ACTION'],\n",
       " ['57 170 40 62 124', '191 39 126 72', 'FALLING_ACTION']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[list_loc['evaluationcrof_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48db62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_filepaths = list(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7861bad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_pairs</th>\n",
       "      <th>seq_label</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>context</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>LINK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>FALLING_ACTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_1</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3;5</td>\n",
       "      <td>0</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3;5_0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_2</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on &lt;ARG1&gt;entering&lt;/ARG1&gt; a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_3</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_4_8</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_4_9</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3;4</td>\n",
       "      <td>43</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3;4_43</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>4;5</td>\n",
       "      <td>26</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_4;5_26</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>Lohan was previously at Betty Ford from Septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3;4</td>\n",
       "      <td>44</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3;4_44</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus               doc_id sent_id eg_id                           index  \\\n",
       "0      esl  1_10ecbplus.xml.xml       3     0     esl_1_10ecbplus.xml.xml_3_0   \n",
       "1      esl  1_10ecbplus.xml.xml       3     1     esl_1_10ecbplus.xml.xml_3_1   \n",
       "2      esl  1_10ecbplus.xml.xml     3;5     0   esl_1_10ecbplus.xml.xml_3;5_0   \n",
       "3      esl  1_10ecbplus.xml.xml       3     2     esl_1_10ecbplus.xml.xml_3_2   \n",
       "4      esl  1_10ecbplus.xml.xml       3     3     esl_1_10ecbplus.xml.xml_3_3   \n",
       "..     ...                  ...     ...   ...                             ...   \n",
       "215    esl  1_10ecbplus.xml.xml       4     8     esl_1_10ecbplus.xml.xml_4_8   \n",
       "216    esl  1_10ecbplus.xml.xml       4     9     esl_1_10ecbplus.xml.xml_4_9   \n",
       "217    esl  1_10ecbplus.xml.xml     3;4    43  esl_1_10ecbplus.xml.xml_3;4_43   \n",
       "218    esl  1_10ecbplus.xml.xml     4;5    26  esl_1_10ecbplus.xml.xml_4;5_26   \n",
       "219    esl  1_10ecbplus.xml.xml     3;4    44  esl_1_10ecbplus.xml.xml_3;4_44   \n",
       "\n",
       "                                                  text  \\\n",
       "0    After skipping out on entering a Newport Beach...   \n",
       "1    After skipping out on entering a Newport Beach...   \n",
       "2    After skipping out on entering a Newport Beach...   \n",
       "3    After skipping out on entering a Newport Beach...   \n",
       "4    After skipping out on entering a Newport Beach...   \n",
       "..                                                 ...   \n",
       "215  Lohan was previously at Betty Ford from Septem...   \n",
       "216  Lohan was previously at Betty Ford from Septem...   \n",
       "217  After skipping out on entering a Newport Beach...   \n",
       "218  Lohan was previously at Betty Ford from Septem...   \n",
       "219  After skipping out on entering a Newport Beach...   \n",
       "\n",
       "                                          text_w_pairs  seq_label  pair_label  \\\n",
       "0    After skipping out on entering a Newport Beach...          1           1   \n",
       "1    After skipping out on entering a Newport Beach...          1           0   \n",
       "2    After skipping out on entering a Newport Beach...          0           0   \n",
       "3    After skipping out on <ARG1>entering</ARG1> a ...          1           0   \n",
       "4    After skipping out on entering a Newport Beach...          1           0   \n",
       "..                                                 ...        ...         ...   \n",
       "215  Lohan was previously at Betty Ford from Septem...          1           0   \n",
       "216  Lohan was previously at Betty Ford from Septem...          1           0   \n",
       "217  After skipping out on entering a Newport Beach...          0           0   \n",
       "218  Lohan was previously at Betty Ford from Septem...          0           0   \n",
       "219  After skipping out on entering a Newport Beach...          0           0   \n",
       "\n",
       "                                               context  num_sents  \\\n",
       "0                                                               1   \n",
       "1                                                               1   \n",
       "2    After skipping out on entering a Newport Beach...          3   \n",
       "3                                                               1   \n",
       "4                                                               1   \n",
       "..                                                 ...        ...   \n",
       "215                                                             1   \n",
       "216                                                             1   \n",
       "217                                                             2   \n",
       "218                                                             2   \n",
       "219                                                             2   \n",
       "\n",
       "               LINK  \n",
       "0    FALLING_ACTION  \n",
       "1              None  \n",
       "2              None  \n",
       "3              None  \n",
       "4              None  \n",
       "..              ...  \n",
       "215            None  \n",
       "216            None  \n",
       "217            None  \n",
       "218            None  \n",
       "219            None  \n",
       "\n",
       "[220 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_s_e_t_list(s_head=[], e_head=[], s_tail=[], e_tail=[]):\n",
    "    s_locs, e_locs, tags = [], [], []\n",
    "    if isinstance(s_head,(np.ndarray,list)):\n",
    "        s_locs.extend(s_head)\n",
    "        e_locs.extend(e_head)\n",
    "        tags.extend(['<ARG0>']*len(s_head))\n",
    "    else:\n",
    "        s_locs.append(s_head)\n",
    "        e_locs.append(e_head)\n",
    "        tags.append('<ARG0>')\n",
    "\n",
    "    if isinstance(s_tail,(np.ndarray,list)):\n",
    "        s_locs.extend(s_tail)\n",
    "        e_locs.extend(e_tail)\n",
    "        tags.extend(['<ARG1>']*len(s_tail))\n",
    "    else:\n",
    "        s_locs.append(s_tail)\n",
    "        e_locs.append(e_tail)\n",
    "        tags.append('<ARG1>')\n",
    "    return sorted(zip(s_locs, e_locs, tags))\n",
    "\n",
    "\n",
    "def run(fn):\n",
    "    # Open File\n",
    "    infos = data_dict[fn]\n",
    "        \n",
    "    # Format Text\n",
    "    sentid_to_text = defaultdict(list)\n",
    "    sentid_to_startloc = {}\n",
    "    word_lengths = []\n",
    "    \n",
    "    for token_infos in infos[list_loc['all_token']]:\n",
    "        sentid = int(token_infos[token_loc['sent_id']])\n",
    "        if sentid not in sentid_to_startloc.keys():\n",
    "            sentid_to_startloc[sentid] = int(token_infos[token_loc['global_loc']])-1\n",
    "        word = token_infos[token_loc['word']]\n",
    "        word_lengths.append(len(word))\n",
    "        sentid_to_text[sentid].append(word)\n",
    "\n",
    "    for k,v in sentid_to_text.items():\n",
    "        sentid_to_text[k]=' '.join(v)\n",
    "\n",
    "    # Format Annotations\n",
    "    coref = infos[list_loc['ecb_coref_relations']]\n",
    "    def get_corefs(k):\n",
    "        return k.split(' ')\n",
    "#         return [k]\n",
    "#         if k in coref.keys():\n",
    "#             return coref[k]\n",
    "#         else:\n",
    "#             return [k]\n",
    "    \n",
    "    rel_to_es = {} # rel_to_es[lid]=(headid,tailid)\n",
    "    causal_rels = []\n",
    "    counter = 0\n",
    "#     # RISING_ACTION == CLINK == PRECONDITION ARE CAUSAL\n",
    "#     for h,t,v in infos[list_loc['evaluationcrof_data']]: #evaluation_data #evaluationcrof_data\n",
    "#         for hs in get_corefs(h):\n",
    "#             for ts in get_corefs(t):\n",
    "# #                 if v in ['PRECONDITION', 'FALLING_ACTION']:\n",
    "# #                     causal_rels.append(counter)\n",
    "# #                 else:\n",
    "# #                     print(h,t,v)\n",
    "#                 rel_to_es[counter]=(hs,ts)\n",
    "#                 causal_rels.append(counter)\n",
    "#                 counter+=1\n",
    "\n",
    "#     for (h,t),v in infos[list_loc['ecbstar_timelink']].items():\n",
    "#         rel_to_es[counter]=(h,t,v)  \n",
    "#         counter+=1\n",
    "                \n",
    "    for (h,t),v in infos[list_loc['ecbstar_events_plotLink']].items():\n",
    "        for hs in get_corefs(h):\n",
    "            tails = get_corefs(t)\n",
    "            for ts in tails:\n",
    "                causal_rels.append(counter)\n",
    "                rel_to_es[counter]=(hs,ts,v[0])\n",
    "                counter+=1\n",
    "            for ts in [i for i in list(infos[list_loc['ecb_star_events']].values()) if i not in tails]:\n",
    "                # interpret not causal\n",
    "                rel_to_es[counter]=(hs,ts,None)\n",
    "                counter+=1\n",
    "        \n",
    "#     for (h,t),v in infos[list_loc['ecbstar_timelink']].items():\n",
    "#         for hs in get_corefs(h):\n",
    "#             for ts in get_corefs(t):\n",
    "#                 rel_to_es[counter]=(hs,ts,v)  \n",
    "#                 counter+=1\n",
    "    \n",
    "    # If not in dict, build in place\n",
    "    def get_e_to_loc(es):\n",
    "        eid = int(es)-1\n",
    "        if es not in eid_to_loc.keys():\n",
    "            e_token = infos[list_loc['all_token']][eid]\n",
    "            e_len = word_lengths[eid]\n",
    "            sentid = int(e_token[token_loc['sent_id']])\n",
    "            start = sum([word_lengths[i]+1 for i in range(sentid_to_startloc[sentid],eid)])\n",
    "            assert(sentid_to_text[sentid][start:start+e_len]==e_token[token_loc['word']])\n",
    "            eid_to_loc[es] = [int(start), int(start+e_len), int(sentid)]\n",
    "        return eid_to_loc[es]\n",
    "\n",
    "    def get_e_to_loc_multi(es):\n",
    "        if es not in eid_to_loc.keys():\n",
    "            if '_' in es:\n",
    "                es_list = es.split('_')\n",
    "            else:\n",
    "                es_list = [es]\n",
    "\n",
    "            _min, _max = np.inf, 0\n",
    "            _sents = []\n",
    "            consequetive_check=True\n",
    "            prev_wordid = -1\n",
    "            for e in es_list:\n",
    "                start, end, sentid = get_e_to_loc(e)\n",
    "                _min = min(start,_min)\n",
    "                _max = max(end,_max)\n",
    "                _sents.append(sentid)\n",
    "                if prev_wordid<0:\n",
    "                    # init\n",
    "                    prev_wordid=int(e)\n",
    "                else:\n",
    "                    if prev_wordid+1!=int(e):\n",
    "                        consequetive_check=False\n",
    "                    prev_wordid=int(e)\n",
    "\n",
    "            _sents = list(set(_sents))\n",
    "            if len(_sents)==0:\n",
    "                raise ValueError('No sentence found.')\n",
    "            elif len(_sents)==1:\n",
    "                _sents = int(_sents[0])\n",
    "            else:\n",
    "                raise ValueError('Argument spans across sentences.')\n",
    "                _sents = f'{min(_sents)};{max(_sents)}'\n",
    "            \n",
    "            if consequetive_check:\n",
    "                eid_to_loc[es] = [_min, _max, _sents]\n",
    "            else:\n",
    "                eid_to_loc[es] = [[eid_to_loc[i][0] for i in es_list], [eid_to_loc[i][1] for i in es_list], _sents]\n",
    "        return eid_to_loc[es]\n",
    "\n",
    "    # Generate Examples\n",
    "    # Retain rels that are in same sentence\n",
    "    corpus = 'esl'\n",
    "    eid_to_loc = {} # init/reset\n",
    "\n",
    "    cols = ['corpus','doc_id','sent_id','eg_id','index','text','text_w_pairs','seq_label','pair_label','context','num_sents']\n",
    "    data = []\n",
    "    sentid_counter = defaultdict(int)\n",
    "\n",
    "    for rel_id, (h, t, v) in rel_to_es.items():\n",
    "        s_head, e_head, sentid_head = get_e_to_loc_multi(h)\n",
    "        s_tail, e_tail, sentid_tail =get_e_to_loc_multi(t)\n",
    "        \n",
    "        if sentid_head == sentid_tail:\n",
    "            sentid = sentid_head\n",
    "            num_eg_for_this_sentid = sentid_counter[sentid]\n",
    "            identifiers = [corpus,os.path.basename(fn),str(sentid),str(num_eg_for_this_sentid)]\n",
    "            unique_index = '_'.join(identifiers)\n",
    "            text = sentid_to_text[sentid]\n",
    "\n",
    "            text_w_pairs = text\n",
    "            s_e_t_list = get_s_e_t_list(s_head=s_head, e_head=e_head, s_tail=s_tail, e_tail=e_tail)\n",
    "            added_t = 0\n",
    "            for s,e,t in s_e_t_list:\n",
    "                text_w_pairs = text_w_pairs[:s+added_t]+t+text_w_pairs[s+added_t:e+added_t]+t[0]+'/'+t[1:]+text_w_pairs[e+added_t:]\n",
    "                added_t+=13\n",
    "            \n",
    "            seq_label = pair_label = 1 if rel_id in causal_rels else 0 \n",
    "\n",
    "            data.append(\n",
    "                identifiers+[\n",
    "                    unique_index,\n",
    "                    text.strip(),\n",
    "                    text_w_pairs.strip(),\n",
    "                    seq_label,\n",
    "                    pair_label,\n",
    "                    '',1,v\n",
    "                ]\n",
    "            )\n",
    "            sentid_counter[sentid]+=1\n",
    "        else:\n",
    "            if sentid_head<0 or sentid_tail<0:\n",
    "                continue\n",
    "            \n",
    "            _min = min(sentid_head,sentid_tail)\n",
    "            _max = max(sentid_head,sentid_tail) \n",
    "            sentid = f'{_min};{_max}'\n",
    "            \n",
    "            num_eg_for_this_sentid = sentid_counter[sentid]\n",
    "            identifiers = [corpus,os.path.basename(fn),str(sentid),str(num_eg_for_this_sentid)]\n",
    "            unique_index = '_'.join(identifiers)\n",
    "            \n",
    "            text_head = sentid_to_text[sentid_head]\n",
    "            text_head_w_pairs = text_head\n",
    "            added_t = 0\n",
    "            for s,e,t in get_s_e_t_list(s_head=s_head, e_head=e_head, s_tail=[], e_tail=[]):\n",
    "                text_head_w_pairs = text_head_w_pairs[:s+added_t]+t+text_head_w_pairs[s+added_t:e+added_t]+t[0]+'/'+t[1:]+text_head_w_pairs[e+added_t:]\n",
    "                added_t+=13\n",
    "            \n",
    "            text_tail = sentid_to_text[sentid_tail]\n",
    "            text_tail_w_pairs = text_tail\n",
    "            added_t = 0\n",
    "            for s,e,t in get_s_e_t_list(s_head=[], e_head=[], s_tail=s_tail, e_tail=e_tail):\n",
    "                text_tail_w_pairs = text_tail_w_pairs[:s+added_t]+t+text_tail_w_pairs[s+added_t:e+added_t]+t[0]+'/'+t[1:]+text_tail_w_pairs[e+added_t:]\n",
    "                added_t+=13\n",
    "            \n",
    "            if abs(sentid_head-sentid_tail)<=1:\n",
    "                context = ''\n",
    "            else:\n",
    "                context = ' '.join([sentid_to_text[s].strip() for s in range(_min, _max+1)])\n",
    "            \n",
    "            if sentid_head<sentid_tail:\n",
    "                text = text_head.strip() + ' ' + text_tail.strip()\n",
    "                text_w_pairs = text_head_w_pairs.strip() + ' ' + text_tail_w_pairs.strip()\n",
    "            else:\n",
    "                text = text_tail.strip() + ' ' + text_head.strip()\n",
    "                text_w_pairs = text_tail_w_pairs.strip() + ' ' + text_head_w_pairs.strip()\n",
    "            \n",
    "            seq_label = pair_label = 1 if rel_id in causal_rels else 0\n",
    "\n",
    "            data.append(\n",
    "                identifiers+[\n",
    "                    unique_index,\n",
    "                    text,\n",
    "                    text_w_pairs,\n",
    "                    seq_label,\n",
    "                    pair_label,\n",
    "                    context,\n",
    "                    _max-_min+1,\n",
    "                    v\n",
    "                ]\n",
    "            )\n",
    "            sentid_counter[sentid]+=1\n",
    "\n",
    "    data = pd.DataFrame(data, columns=cols+['LINK'])\n",
    "    data['seq_label'] = data.groupby(['corpus','doc_id','sent_id'])['seq_label'].transform('max')\n",
    "    data['sent_id'] = data['sent_id'].astype(str)\n",
    "    \n",
    "    return data\n",
    "\n",
    "run(list_of_filepaths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc1e4522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_pairs</th>\n",
       "      <th>seq_label</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>context</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>LINK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>FALLING_ACTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_1</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3;5</td>\n",
       "      <td>0</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3;5_0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_2</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on &lt;ARG1&gt;entering&lt;/ARG1&gt; a ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esl</td>\n",
       "      <td>1_10ecbplus.xml.xml</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>esl_1_10ecbplus.xml.xml_3_3</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>After skipping out on entering a Newport Beach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39129</th>\n",
       "      <td>esl</td>\n",
       "      <td>8_9ecbplus.xml.xml</td>\n",
       "      <td>1;8</td>\n",
       "      <td>34</td>\n",
       "      <td>esl_8_9ecbplus.xml.xml_1;8_34</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' &lt;ARG1...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39130</th>\n",
       "      <td>esl</td>\n",
       "      <td>8_9ecbplus.xml.xml</td>\n",
       "      <td>1;8</td>\n",
       "      <td>35</td>\n",
       "      <td>esl_8_9ecbplus.xml.xml_1;8_35</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39131</th>\n",
       "      <td>esl</td>\n",
       "      <td>8_9ecbplus.xml.xml</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>esl_8_9ecbplus.xml.xml_8_27</td>\n",
       "      <td>Three people died when an Athens bank went up ...</td>\n",
       "      <td>Three people died when an Athens bank went up ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39132</th>\n",
       "      <td>esl</td>\n",
       "      <td>8_9ecbplus.xml.xml</td>\n",
       "      <td>3;8</td>\n",
       "      <td>62</td>\n",
       "      <td>esl_8_9ecbplus.xml.xml_3;8_62</td>\n",
       "      <td>The Greek prime minister has denounced what he...</td>\n",
       "      <td>The Greek prime minister has denounced what he...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Greek prime minister has denounced what he...</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39133</th>\n",
       "      <td>esl</td>\n",
       "      <td>8_9ecbplus.xml.xml</td>\n",
       "      <td>1;8</td>\n",
       "      <td>36</td>\n",
       "      <td>esl_8_9ecbplus.xml.xml_1;8_36</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>Greek Prime Minister &lt;ARG1&gt;Calls&lt;/ARG1&gt; Bank W...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greek Prime Minister Calls Bank Workers' Riot ...</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39134 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      corpus               doc_id sent_id eg_id  \\\n",
       "0        esl  1_10ecbplus.xml.xml       3     0   \n",
       "1        esl  1_10ecbplus.xml.xml       3     1   \n",
       "2        esl  1_10ecbplus.xml.xml     3;5     0   \n",
       "3        esl  1_10ecbplus.xml.xml       3     2   \n",
       "4        esl  1_10ecbplus.xml.xml       3     3   \n",
       "...      ...                  ...     ...   ...   \n",
       "39129    esl   8_9ecbplus.xml.xml     1;8    34   \n",
       "39130    esl   8_9ecbplus.xml.xml     1;8    35   \n",
       "39131    esl   8_9ecbplus.xml.xml       8    27   \n",
       "39132    esl   8_9ecbplus.xml.xml     3;8    62   \n",
       "39133    esl   8_9ecbplus.xml.xml     1;8    36   \n",
       "\n",
       "                               index  \\\n",
       "0        esl_1_10ecbplus.xml.xml_3_0   \n",
       "1        esl_1_10ecbplus.xml.xml_3_1   \n",
       "2      esl_1_10ecbplus.xml.xml_3;5_0   \n",
       "3        esl_1_10ecbplus.xml.xml_3_2   \n",
       "4        esl_1_10ecbplus.xml.xml_3_3   \n",
       "...                              ...   \n",
       "39129  esl_8_9ecbplus.xml.xml_1;8_34   \n",
       "39130  esl_8_9ecbplus.xml.xml_1;8_35   \n",
       "39131    esl_8_9ecbplus.xml.xml_8_27   \n",
       "39132  esl_8_9ecbplus.xml.xml_3;8_62   \n",
       "39133  esl_8_9ecbplus.xml.xml_1;8_36   \n",
       "\n",
       "                                                    text  \\\n",
       "0      After skipping out on entering a Newport Beach...   \n",
       "1      After skipping out on entering a Newport Beach...   \n",
       "2      After skipping out on entering a Newport Beach...   \n",
       "3      After skipping out on entering a Newport Beach...   \n",
       "4      After skipping out on entering a Newport Beach...   \n",
       "...                                                  ...   \n",
       "39129  Greek Prime Minister Calls Bank Workers' Riot ...   \n",
       "39130  Greek Prime Minister Calls Bank Workers' Riot ...   \n",
       "39131  Three people died when an Athens bank went up ...   \n",
       "39132  The Greek prime minister has denounced what he...   \n",
       "39133  Greek Prime Minister Calls Bank Workers' Riot ...   \n",
       "\n",
       "                                            text_w_pairs  seq_label  \\\n",
       "0      After skipping out on entering a Newport Beach...        1.0   \n",
       "1      After skipping out on entering a Newport Beach...        1.0   \n",
       "2      After skipping out on entering a Newport Beach...        0.0   \n",
       "3      After skipping out on <ARG1>entering</ARG1> a ...        1.0   \n",
       "4      After skipping out on entering a Newport Beach...        1.0   \n",
       "...                                                  ...        ...   \n",
       "39129  Greek Prime Minister Calls Bank Workers' <ARG1...        0.0   \n",
       "39130  Greek Prime Minister Calls Bank Workers' Riot ...        0.0   \n",
       "39131  Three people died when an Athens bank went up ...        1.0   \n",
       "39132  The Greek prime minister has denounced what he...        0.0   \n",
       "39133  Greek Prime Minister <ARG1>Calls</ARG1> Bank W...        0.0   \n",
       "\n",
       "      pair_label                                            context num_sents  \\\n",
       "0              1                                                            1   \n",
       "1              0                                                            1   \n",
       "2              0  After skipping out on entering a Newport Beach...         3   \n",
       "3              0                                                            1   \n",
       "4              0                                                            1   \n",
       "...          ...                                                ...       ...   \n",
       "39129          0  Greek Prime Minister Calls Bank Workers' Riot ...         8   \n",
       "39130          0  Greek Prime Minister Calls Bank Workers' Riot ...         8   \n",
       "39131          0                                                            1   \n",
       "39132          0  The Greek prime minister has denounced what he...         6   \n",
       "39133          0  Greek Prime Minister Calls Bank Workers' Riot ...         8   \n",
       "\n",
       "                 LINK  \n",
       "0      FALLING_ACTION  \n",
       "1                None  \n",
       "2                None  \n",
       "3                None  \n",
       "4                None  \n",
       "...               ...  \n",
       "39129            None  \n",
       "39130            None  \n",
       "39131            None  \n",
       "39132            None  \n",
       "39133            None  \n",
       "\n",
       "[39134 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for counter, fn in enumerate(list_of_filepaths):\n",
    "    df = run(fn)\n",
    "    data = pd.concat([data, df])\n",
    "data = data.drop_duplicates(subset='text_w_pairs').reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89752bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/49 ECI/crossdomain_data/EventStoryLine/annotated_data/v1.0/8/8_9ecbplus.xml.xml'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "152d66b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After skipping out on entering a Newport Beach rehabilitation facility and facing the prospect of <ARG0>arrest</ARG0> for <ARG1>violating</ARG1> her probation , Lindsay Lohan has checked into the Betty Ford Center to begin a 90 - day court - mandated stay in her reckless driving conviction .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['text_w_pairs'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0db56a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'14_10ecbplus.xml.xml',\n",
       " '1_17ecbplus.xml.xml',\n",
       " '1_6ecbplus.xml.xml',\n",
       " '4_6ecbplus.xml.xml'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Files\n",
    "set([os.path.basename(k) for k in data_dict.keys()])-set(data['doc_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d2f4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Examples\n",
      "Seq Level: Counter({0.0: 27740, 1.0: 11394})\n",
      "Pair Level: Counter({0: 37586, 1: 1548})\n",
      "Seq Level (Unique): Counter({0.0: 3346, 1.0: 1310})\n",
      "\n",
      "Single Sentence Examples\n",
      "Seq Level: Counter({1.0: 6751, 0.0: 389})\n",
      "Pair Level: Counter({0: 5889, 1: 1251})\n",
      "Seq Level (Unique): Counter({1.0: 835, 0.0: 142})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('All Examples')\n",
    "print('Seq Level:', Counter(data['seq_label'])) # if sentence level causality exists\n",
    "print('Pair Level:',Counter(data['pair_label'])) # if ARG0-ARG1 pair level causality exists\n",
    "print('Seq Level (Unique):',Counter(data.drop_duplicates(subset=['corpus','doc_id','sent_id'])['seq_label']))\n",
    "\n",
    "print('\\nSingle Sentence Examples')\n",
    "print('Seq Level:', Counter(data.loc[data['num_sents']==1,'seq_label'])) # if sentence level causality exists\n",
    "print('Pair Level:', Counter(data.loc[data['num_sents']==1,'pair_label'])) # if ARG0-ARG1 pair level causality exists\n",
    "print('Seq Level (Unique):',Counter(data.loc[data['num_sents']==1].drop_duplicates(subset=['corpus','doc_id','sent_id'])['seq_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16bc879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned/esl2.csv', index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a633b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38e910",
   "metadata": {},
   "source": [
    "### V0.9\n",
    "\n",
    "#### Without Coref\n",
    "    All Examples\n",
    "    Seq Level: Counter({1.0: 3561, 0.0: 2846})\n",
    "    Pair Level: Counter({0: 4151, 1: 2256})\n",
    "    Seq Level (Unique): Counter({1.0: 1335, 0.0: 949})\n",
    "\n",
    "    Single Sentence Examples\n",
    "    Seq Level: Counter({1.0: 2618, 0.0: 166})\n",
    "    Pair Level: Counter({1: 1598, 0: 1186})\n",
    "    Seq Level (Unique): Counter({1.0: 864, 0.0: 69})\n",
    "\n",
    "\n",
    "#### With Coref\n",
    "    All Examples\n",
    "    Seq Level: Counter({1.0: 13125, 0.0: 3575})\n",
    "    Pair Level: Counter({1: 9300, 0: 7400})\n",
    "    Seq Level (Unique): Counter({1.0: 2485, 0.0: 756})\n",
    "\n",
    "    Single Sentence Examples\n",
    "    Seq Level: Counter({1.0: 4157, 0.0: 199})\n",
    "    Pair Level: Counter({1: 2717, 0: 1639})\n",
    "    Seq Level (Unique): Counter({1.0: 897, 0.0: 64})\n",
    "\n",
    "\n",
    "\n",
    "### V1.0\n",
    "\n",
    "#### Without Coref\n",
    "    All Examples\n",
    "    Seq Level: Counter({1.0: 3632, 0.0: 2501})\n",
    "    Pair Level: Counter({0: 3854, 1: 2279})\n",
    "    Seq Level (Unique): Counter({1.0: 1349, 0.0: 915})\n",
    "\n",
    "    Single Sentence Examples\n",
    "    Seq Level: Counter({1.0: 2753, 0.0: 193})\n",
    "    Pair Level: Counter({1: 1609, 0: 1337})\n",
    "    Seq Level (Unique): Counter({1.0: 869, 0.0: 81})\n",
    "\n",
    "#### With Coref\n",
    "    All Examples\n",
    "    Seq Level: Counter({1.0: 12888, 0.0: 3441})\n",
    "    Pair Level: Counter({1: 9244, 0: 7085})\n",
    "    Seq Level (Unique): Counter({1.0: 2489, 0.0: 732})\n",
    "\n",
    "    Single Sentence Examples\n",
    "    Seq Level: Counter({1.0: 4257, 0.0: 212})\n",
    "    Pair Level: Counter({1: 2720, 0: 1749})\n",
    "    Seq Level (Unique): Counter({1.0: 901, 0.0: 75})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62892c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

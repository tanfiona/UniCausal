{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4127b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "D:\\59 Github Projects\\0015 BECAUSE\\BECAUSE-2.0\\CongressionalHearings\\CHRG-110hhrg44900-1.ann\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "folder = r\"D:\\59 Github Projects\\0015 BECAUSE\\BECAUSE-2.0\"\n",
    "list_of_filepaths = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ann\"):\n",
    "             list_of_filepaths.append(os.path.join(root, file))\n",
    "print(len(list_of_filepaths))\n",
    "print(list_of_filepaths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bc015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\59 Github Projects\\0015 BECAUSE\\BECAUSE-2.0\\MASC\\20000410_nyt-NEW.txt\n",
      "D:\\08 Thesis\\09 Datasets\\07. PennTreeBank V3.0\\PDTB-3.0\\data\\raw\\21\\wsj_2125\n",
      "D:\\08 Thesis\\09 Datasets\\07. PennTreeBank V3.0\\PDTB-3.0\\data\\raw\\20\\wsj_2004_e\n"
     ]
    }
   ],
   "source": [
    "def get_txt_fp_from_ann_fp(fn):\n",
    "    if 'PTB' in fn:\n",
    "        pdtb_dir = r\"D:\\08 Thesis\\09 Datasets\\07. PennTreeBank V3.0\\PDTB-3.0\\data\\raw\"\n",
    "        pdtb_num = os.path.basename(fn).split('_')[-1].split('.')[0]\n",
    "        if int(pdtb_num) in [1506, 2004]: #EXCEPTIONS. MANUALLY EDITTED LINE SPACING\n",
    "            return os.path.join(pdtb_dir, pdtb_num[0:2], f'wsj_{pdtb_num}_e')\n",
    "        else:\n",
    "            return os.path.join(pdtb_dir, pdtb_num[0:2], f'wsj_{pdtb_num}')\n",
    "    else:\n",
    "        return os.path.splitext(fn)[0]+'.txt'\n",
    "    \n",
    "print(get_txt_fp_from_ann_fp('D:\\\\59 Github Projects\\\\0015 BECAUSE\\\\BECAUSE-2.0\\\\MASC\\\\20000410_nyt-NEW.ann'))    \n",
    "print(get_txt_fp_from_ann_fp('D:\\\\59 Github Projects\\\\0015 BECAUSE\\\\BECAUSE-2.0\\\\PTB\\\\wsj_2125.ann'))\n",
    "print(get_txt_fp_from_ann_fp('D:\\\\59 Github Projects\\\\0015 BECAUSE\\\\BECAUSE-2.0\\\\PTB\\\\wsj_2004.ann'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb82681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\effbl\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"\n",
      "C:\\Users\\effbl\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:86: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_pairs</th>\n",
       "      <th>seq_label</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>context</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_7_0</td>\n",
       "      <td>Earlier this year, bankers and other investors...</td>\n",
       "      <td>Earlier this year, &lt;ARG1&gt;bankers and other inv...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_9_0</td>\n",
       "      <td>\"Competition from third parties who have cash ...</td>\n",
       "      <td>\"Competition from third parties who have cash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_6_0</td>\n",
       "      <td>\"The pricing will become more realistic, which...</td>\n",
       "      <td>\"The pricing will become more realistic, &lt;ARG0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_11_0</td>\n",
       "      <td>At Saks Fifth Avenue, Paul Leblang, senior vic...</td>\n",
       "      <td>At Saks Fifth Avenue, Paul Leblang, senior vic...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_12_0</td>\n",
       "      <td>\"Having to take on less debt would certainly b...</td>\n",
       "      <td>\"Having to take on less debt would certainly b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_13_0</td>\n",
       "      <td>To make an LBO work, now we are going to need ...</td>\n",
       "      <td>To &lt;ARG1&gt;make an LBO work&lt;/ARG1&gt;, now we are g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>because_wsj_2125.ann_13_1</td>\n",
       "      <td>To make an LBO work, now we are going to need ...</td>\n",
       "      <td>To make &lt;ARG1&gt;an LBO work&lt;/ARG1&gt;, now &lt;ARG0&gt;we...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_14_0</td>\n",
       "      <td>Not only could the Wall Street gyrations damp ...</td>\n",
       "      <td>Not only &lt;ARG1&gt;could the Wall Street gyrations...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_16_0</td>\n",
       "      <td>However, the lower prices these retail chains ...</td>\n",
       "      <td>However, &lt;ARG0&gt;the lower prices these retail c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_18_0</td>\n",
       "      <td>\"What's encouraging about this is that retail ...</td>\n",
       "      <td>\"What's encouraging about this is that retail ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_19_0</td>\n",
       "      <td>Still, most retailing observers expect that al...</td>\n",
       "      <td>Still, most retailing observers expect that al...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_20_0</td>\n",
       "      <td>\"Prices for retail chains are lower today than...</td>\n",
       "      <td>\"Prices for retail chains are lower today than...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     corpus        doc_id sent_id eg_id                      index  \\\n",
       "0   because  wsj_2125.ann       7     0   because_wsj_2125.ann_7_0   \n",
       "1   because  wsj_2125.ann       9     0   because_wsj_2125.ann_9_0   \n",
       "2   because  wsj_2125.ann       6     0   because_wsj_2125.ann_6_0   \n",
       "3   because  wsj_2125.ann      11     0  because_wsj_2125.ann_11_0   \n",
       "4   because  wsj_2125.ann      12     0  because_wsj_2125.ann_12_0   \n",
       "5   because  wsj_2125.ann      13     0  because_wsj_2125.ann_13_0   \n",
       "6   because  wsj_2125.ann      13     1  because_wsj_2125.ann_13_1   \n",
       "7   because  wsj_2125.ann      14     0  because_wsj_2125.ann_14_0   \n",
       "8   because  wsj_2125.ann      16     0  because_wsj_2125.ann_16_0   \n",
       "9   because  wsj_2125.ann      18     0  because_wsj_2125.ann_18_0   \n",
       "10  because  wsj_2125.ann      19     0  because_wsj_2125.ann_19_0   \n",
       "11  because  wsj_2125.ann      20     0  because_wsj_2125.ann_20_0   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Earlier this year, bankers and other investors...   \n",
       "1   \"Competition from third parties who have cash ...   \n",
       "2   \"The pricing will become more realistic, which...   \n",
       "3   At Saks Fifth Avenue, Paul Leblang, senior vic...   \n",
       "4   \"Having to take on less debt would certainly b...   \n",
       "5   To make an LBO work, now we are going to need ...   \n",
       "6   To make an LBO work, now we are going to need ...   \n",
       "7   Not only could the Wall Street gyrations damp ...   \n",
       "8   However, the lower prices these retail chains ...   \n",
       "9   \"What's encouraging about this is that retail ...   \n",
       "10  Still, most retailing observers expect that al...   \n",
       "11  \"Prices for retail chains are lower today than...   \n",
       "\n",
       "                                         text_w_pairs  seq_label  pair_label  \\\n",
       "0   Earlier this year, <ARG1>bankers and other inv...          1           1   \n",
       "1   \"Competition from third parties who have cash ...          1           1   \n",
       "2   \"The pricing will become more realistic, <ARG0...          1           1   \n",
       "3   At Saks Fifth Avenue, Paul Leblang, senior vic...          1           1   \n",
       "4   \"Having to take on less debt would certainly b...          1           1   \n",
       "5   To <ARG1>make an LBO work</ARG1>, now we are g...          1           1   \n",
       "6   To make <ARG1>an LBO work</ARG1>, now <ARG0>we...          1           1   \n",
       "7   Not only <ARG1>could the Wall Street gyrations...          1           1   \n",
       "8   However, <ARG0>the lower prices these retail c...          1           1   \n",
       "9   \"What's encouraging about this is that retail ...          0           0   \n",
       "10  Still, most retailing observers expect that al...          1           1   \n",
       "11  \"Prices for retail chains are lower today than...          1           1   \n",
       "\n",
       "   context  num_sents  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  \n",
       "10                  1  \n",
       "11                  1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def parse_ann_file(fn):\n",
    "    \n",
    "    with open(fn, 'rU') as f:\n",
    "        annotations = f.readlines()\n",
    "    \n",
    "    arguments = {} #T\n",
    "    relations = {} #E\n",
    "    attributes = {} #A\n",
    "    notes = []\n",
    "\n",
    "    for ann in annotations:\n",
    "        ann = ann.strip().split('\\t')\n",
    "        index_, info = ann[0], ann[1]\n",
    "\n",
    "        # notes to ignore for now\n",
    "        if 'AnnotatorNotes' in info:\n",
    "            notes.append(ann)\n",
    "            continue\n",
    "\n",
    "        # ARGUMENTS\n",
    "        if index_[0]=='T':\n",
    "            text_ = ann[2]\n",
    "\n",
    "            # if annotation is discontinuous\n",
    "            # e.g. \"Consequence 7824 7833;7837 7843\"\n",
    "            info_b = None\n",
    "            if ';' in info:\n",
    "                info = info.split(';')\n",
    "                info_b = info[1:] # back info\n",
    "                info = info[0] # main/front info\n",
    "\n",
    "            type_, start, end = info.split(' ')\n",
    "            loc = [(int(start), int(end))]\n",
    "\n",
    "            if info_b is not None:\n",
    "                for i in info_b:\n",
    "                    start, end = i.split(' ')\n",
    "                    loc.append((int(start), int(end)))\n",
    "\n",
    "            arguments[index_] = {\n",
    "                'type': type_,\n",
    "                'loc': loc,\n",
    "                'text': text_\n",
    "            }\n",
    "\n",
    "        # ATTRIBUTES\n",
    "        elif index_[0]=='A':\n",
    "            info = info.split(' ')\n",
    "            if len(info)==2:\n",
    "                attributes[index_] = {\n",
    "                    'type': info[0],\n",
    "                    'index': info[1]\n",
    "                }\n",
    "            elif len(info)==3:\n",
    "                attributes[index_] = {\n",
    "                    'type': info[0],\n",
    "                    'index': info[1],\n",
    "                    'value': info[2]\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "        # RELATIONS\n",
    "        elif index_[0]=='E':\n",
    "            info = info.split(' ')\n",
    "            relations[index_] = {}\n",
    "            for arg_info in info[1:]:\n",
    "                argname, argindex = arg_info.split(':')\n",
    "                relations[index_][argname] = argindex\n",
    "\n",
    "    # reverse index to allow search by relation id (#E format)\n",
    "    r_attributes = defaultdict(dict)\n",
    "    for k,v in attributes.items():\n",
    "        r_attributes[v['index']][k]=v\n",
    "\n",
    "    return arguments, relations, attributes, r_attributes\n",
    "\n",
    "\n",
    "def split_doc_into_sentences(doc):\n",
    "    return nltk.sent_tokenize(doc)\n",
    "\n",
    "\n",
    "def parse_txt_file(fn, buffer = 10):\n",
    "    with open(get_txt_fp_from_ann_fp(fn), 'rU') as f:\n",
    "        document_ = f.readlines()\n",
    "\n",
    "    sent2locid = {-1:0}\n",
    "    document = ''\n",
    "    sent_id = 0\n",
    "\n",
    "    if 'PTB' in fn: # alr split by lines\n",
    "        sents = []\n",
    "        for doc in document_:\n",
    "            doc=re.sub('\\n','',doc)\n",
    "            document+=str(doc)+' '\n",
    "            if doc!='':\n",
    "                sents.append(doc)\n",
    "    else:\n",
    "        for doc in document_:\n",
    "            doc=re.sub('\\n','',doc)\n",
    "            document+=str(doc)+' '\n",
    "        document = document[:-1]\n",
    "        sents = split_doc_into_sentences(document)\n",
    "    \n",
    "    for sent in sents:\n",
    "        if sent=='':\n",
    "            sent2locid[sent_id-1]+=1\n",
    "            continue\n",
    "        start_from=max(0,sent2locid[sent_id-1]-buffer)\n",
    "        sent2locid[sent_id]=re.search(re.escape(sent),document[start_from:]).end()+1+start_from\n",
    "        sent_id+=1\n",
    "    \n",
    "    sent2locid[max(sent2locid.keys())+1]=len(document)\n",
    "\n",
    "    return document[:-1], sent2locid\n",
    "    \n",
    "    \n",
    "def find_sents_needed(sent2locid, search_min, search_max):\n",
    "    sents_needed = []\n",
    "    for k,v in sent2locid.items():\n",
    "        if v<=search_min:\n",
    "            # keep replacing as first item\n",
    "            sents_needed = [k+1]\n",
    "            continue\n",
    "        if v>search_max:\n",
    "            # sufficient found, exit\n",
    "            break\n",
    "        sents_needed.append(k+1)\n",
    "    return sents_needed\n",
    "    \n",
    "    \n",
    "def readjust_arguments(document, arguments, buffer=30):\n",
    "    \n",
    "    new_arguments = {}\n",
    "\n",
    "    for a_id, a_v in arguments.items():\n",
    "\n",
    "        # get new values\n",
    "        s = 0\n",
    "        new_loc = []\n",
    "        for (start, end) in a_v['loc']:\n",
    "\n",
    "            length = end-start\n",
    "            start_from = max(0,start-buffer)\n",
    "            search_result = re.search(re.escape(a_v['text'][s:s+length]),document[start_from:])\n",
    "            if search_result is None:\n",
    "                text = a_v['text'][s:s+length]\n",
    "                text = re.sub(\"`\",\"'\",text)\n",
    "                text = re.sub(\"('')\",'\"',text)\n",
    "                search_result = re.search(re.escape(text),document[start_from:])\n",
    "                a_v['text'] = text\n",
    "            new_start, new_end = search_result.start(), search_result.end()        \n",
    "            new_loc.append((new_start+start_from, new_end+start_from))\n",
    "\n",
    "            # prepare for next iteration\n",
    "            s += length+1\n",
    "\n",
    "        # update new values\n",
    "        a_v['loc'] = new_loc\n",
    "        new_arguments[a_id] = a_v\n",
    "        \n",
    "    return new_arguments\n",
    "\n",
    "\n",
    "def get_s_e_t_list(s_head=[], e_head=[], s_tail=[], e_tail=[]):\n",
    "    s_locs, e_locs, tags = [], [], []\n",
    "    if isinstance(s_head,(np.ndarray,list)):\n",
    "        s_locs.extend(s_head)\n",
    "        e_locs.extend(e_head)\n",
    "        tags.extend(['<ARG0>']*len(s_head))\n",
    "    else:\n",
    "        s_locs.append(s_head)\n",
    "        e_locs.append(e_head)\n",
    "        tags.append('<ARG0>')\n",
    "\n",
    "    if isinstance(s_tail,(np.ndarray,list)):\n",
    "        s_locs.extend(s_tail)\n",
    "        e_locs.extend(e_tail)\n",
    "        tags.extend(['<ARG1>']*len(s_tail))\n",
    "    else:\n",
    "        s_locs.append(s_tail)\n",
    "        e_locs.append(e_tail)\n",
    "        tags.append('<ARG1>')\n",
    "    return sorted(zip(s_locs, e_locs, tags))\n",
    "\n",
    "\n",
    "def run(fn):\n",
    "    arguments, relations, attributes, r_attributes = parse_ann_file(fn)\n",
    "    document, sent2locid = parse_txt_file(os.path.splitext(fn)[0]+'.txt')\n",
    "    if 'PTB' in fn:\n",
    "        for buffer in [15,30,50]:\n",
    "            try:\n",
    "                arguments = readjust_arguments(document, arguments, buffer)\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Format Text\n",
    "    sentid_to_text = {}\n",
    "    for k,v in sent2locid.items():\n",
    "        if k<0:\n",
    "            continue\n",
    "        else:\n",
    "            sentid_to_text[k]=document[sent2locid[k-1]:v]\n",
    "\n",
    "    # Get eid_to_loc\n",
    "    eid_to_loc = defaultdict(list)\n",
    "\n",
    "    for k,v in arguments.items():\n",
    "        ss, ee = [], []\n",
    "        prev_e = np.inf\n",
    "        for s,e in v['loc']:\n",
    "            if prev_e+1==s: # consecutive\n",
    "                ee.pop()\n",
    "                ee.append(int(e))\n",
    "            else:\n",
    "                ss.append(int(s))\n",
    "                ee.append(int(e))\n",
    "            prev_e = int(e)\n",
    "\n",
    "        _sents = find_sents_needed(sent2locid, min(ss), max(ss))\n",
    "        if len(_sents)==0:\n",
    "            raise ValueError('No sentence found.')\n",
    "        elif len(_sents)==1:\n",
    "            _sents = int(_sents[0])\n",
    "        else:\n",
    "            raise ValueError('Argument spans across sentences.')\n",
    "            _sents = f'{min(_sents)};{max(_sents)}'\n",
    "\n",
    "        eid_to_loc[k] = [ss,ee,_sents]\n",
    "        \n",
    "    # Format Annotations\n",
    "    rel_to_es = {} # rel_to_es[lid]=(headid,tailid)\n",
    "    causal_rels = []\n",
    "    counter = 0\n",
    "\n",
    "    for k,v in relations.items():\n",
    "        if ('Arg0' in v.keys() and 'Arg1' in v.keys()):\n",
    "            rel_to_es[k]=(v['Arg0'],v['Arg1'])\n",
    "        elif ('Cause' in v.keys() and 'Effect' in v.keys()):\n",
    "            causal_rels.append(k)\n",
    "            rel_to_es[k]=(v['Cause'],v['Effect'])\n",
    "        \n",
    "    # Generate Examples\n",
    "    # Retain rels that are in same sentence\n",
    "    corpus = 'because'\n",
    "\n",
    "    cols = ['corpus','doc_id','sent_id','eg_id','index','text','text_w_pairs','seq_label','pair_label','context','num_sents']\n",
    "    data = []\n",
    "    sentid_counter = defaultdict(int)\n",
    "\n",
    "    for rel_id, (h, t) in rel_to_es.items():\n",
    "        s_head, e_head, sentid_head = eid_to_loc[h]\n",
    "        s_tail, e_tail, sentid_tail = eid_to_loc[t]\n",
    "        if sentid_head == sentid_tail:\n",
    "            sentid = sentid_head\n",
    "            num_eg_for_this_sentid = sentid_counter[sentid]\n",
    "            identifiers = [corpus,os.path.basename(fn),str(sentid),str(num_eg_for_this_sentid)]\n",
    "            unique_index = '_'.join(identifiers)\n",
    "            text = sentid_to_text[sentid]\n",
    "\n",
    "            text_w_pairs = text\n",
    "            added_t = 0\n",
    "            accounting = sent2locid[sentid-1]\n",
    "            for s,e,t in get_s_e_t_list(s_head=s_head, e_head=e_head, s_tail=s_tail, e_tail=e_tail):\n",
    "                s = s+added_t-accounting\n",
    "                e = e+added_t-accounting\n",
    "                text_w_pairs = text_w_pairs[:s]+t+text_w_pairs[s:e]+t[0]+'/'+t[1:]+text_w_pairs[e:]\n",
    "                added_t+=13\n",
    "\n",
    "            seq_label = pair_label = 1 if rel_id in causal_rels else 0\n",
    "\n",
    "            data.append(\n",
    "                identifiers+[\n",
    "                    unique_index,\n",
    "                    text.strip(),\n",
    "                    text_w_pairs.strip(),\n",
    "                    seq_label,\n",
    "                    pair_label,\n",
    "                    '',1\n",
    "                ]\n",
    "            )\n",
    "            sentid_counter[sentid]+=1\n",
    "        else:\n",
    "            raise ValueError('BECAUSE Dataset does not have inter-sentence examples!')\n",
    "#             if sentid_head<0 or sentid_tail<0:\n",
    "#                 continue\n",
    "            \n",
    "#             _min = min(sentid_head,sentid_tail)\n",
    "#             _max = max(sentid_head,sentid_tail) \n",
    "#             sentid = f'{_min};{_max}'\n",
    "            \n",
    "#             num_eg_for_this_sentid = sentid_counter[sentid]\n",
    "#             identifiers = [corpus,os.path.basename(fn),str(sentid),str(num_eg_for_this_sentid)]\n",
    "#             unique_index = '_'.join(identifiers)\n",
    "            \n",
    "#             text_head = sentid_to_text[sentid_head]\n",
    "#             text_head_w_pairs = text_head\n",
    "#             added_t = 0\n",
    "#             accounting = sent2locid[sentid_head-1]\n",
    "#             for s,e,t in get_s_e_t_list(s_head=s_head, e_head=e_head, s_tail=[], e_tail=[]):\n",
    "#                 s = s+added_t-accounting\n",
    "#                 e = e+added_t-accounting\n",
    "#                 text_head_w_pairs = text_head_w_pairs[:s]+t+text_head_w_pairs[s:e]+t[0]+'/'+t[1:]+text_head_w_pairs[e:]\n",
    "#                 added_t+=13\n",
    "            \n",
    "#             text_tail = sentid_to_text[sentid_tail]\n",
    "#             text_tail_w_pairs = text_tail\n",
    "#             added_t = 0\n",
    "#             accounting = sent2locid[sentid_tail-1]\n",
    "#             for s,e,t in get_s_e_t_list(s_head=[], e_head=[], s_tail=s_tail, e_tail=e_tail):\n",
    "#                 s = s+added_t-accounting\n",
    "#                 e = e+added_t-accounting\n",
    "#                 text_tail_w_pairs = text_tail_w_pairs[:s]+t+text_tail_w_pairs[s:e]+t[0]+'/'+t[1:]+text_tail_w_pairs[e:]\n",
    "#                 added_t+=13\n",
    "            \n",
    "#             if abs(sentid_head-sentid_tail)<=1:\n",
    "#                 context = ''\n",
    "#             else:\n",
    "#                 context = ' '.join([sentid_to_text[s].strip() for s in range(_min, _max+1)])\n",
    "            \n",
    "#             if sentid_head<sentid_tail:\n",
    "#                 text = text_head.strip() + ' ' + text_tail.strip()\n",
    "#                 text_w_pairs = text_head_w_pairs.strip() + ' ' + text_tail_w_pairs.strip()\n",
    "#             else:\n",
    "#                 text = text_tail.strip() + ' ' + text_head.strip()\n",
    "#                 text_w_pairs = text_tail_w_pairs.strip() + ' ' + text_head_w_pairs.strip()\n",
    "            \n",
    "#             seq_label = pair_label = 1 if rel_id in causal_rels else 0 \n",
    "\n",
    "#             data.append(\n",
    "#                 identifiers+[\n",
    "#                     unique_index,\n",
    "#                     text,\n",
    "#                     text_w_pairs,\n",
    "#                     seq_label,\n",
    "#                     pair_label,\n",
    "#                     context,\n",
    "#                     _max-_min+1\n",
    "#                 ]\n",
    "#             )\n",
    "#             sentid_counter[sentid]+=1\n",
    "\n",
    "    data = pd.DataFrame(data, columns=cols)\n",
    "    data['sent_id'] = data['sent_id'].astype(str)\n",
    "    data['seq_label'] = data.groupby(['corpus','doc_id','sent_id'])['seq_label'].transform('max')\n",
    "        \n",
    "    return data   \n",
    "    \n",
    "fn = 'D:\\\\59 Github Projects\\\\0015 BECAUSE\\\\BECAUSE-2.0\\\\PTB\\\\wsj_2125.ann'\n",
    "run(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0370c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\effbl\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"\n",
      "C:\\Users\\effbl\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:86: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_pairs</th>\n",
       "      <th>seq_label</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>context</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>because</td>\n",
       "      <td>CHRG-110hhrg44900-1.ann</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>because_CHRG-110hhrg44900-1.ann_5_0</td>\n",
       "      <td>We have gotten the agreement of the Chairman a...</td>\n",
       "      <td>We have gotten the agreement of the Chairman a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because</td>\n",
       "      <td>CHRG-110hhrg44900-1.ann</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>because_CHRG-110hhrg44900-1.ann_13_0</td>\n",
       "      <td>Members want to get a new context because the ...</td>\n",
       "      <td>&lt;ARG1&gt;Members want to get a new context&lt;/ARG1&gt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>because</td>\n",
       "      <td>CHRG-110hhrg44900-1.ann</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>because_CHRG-110hhrg44900-1.ann_15_0</td>\n",
       "      <td>Given the importance of this, and given the in...</td>\n",
       "      <td>Given &lt;ARG0&gt;the importance of this&lt;/ARG0&gt;, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>because</td>\n",
       "      <td>CHRG-110hhrg44900-1.ann</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>because_CHRG-110hhrg44900-1.ann_25_0</td>\n",
       "      <td>Where there was a strong argument as recently ...</td>\n",
       "      <td>Where there was a strong argument as recently ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>because</td>\n",
       "      <td>CHRG-110hhrg44900-1.ann</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>because_CHRG-110hhrg44900-1.ann_38_0</td>\n",
       "      <td>Something that simple causes problems in subpr...</td>\n",
       "      <td>Something that simple causes problems in subpr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_14_0</td>\n",
       "      <td>Not only could the Wall Street gyrations damp ...</td>\n",
       "      <td>Not only &lt;ARG1&gt;could the Wall Street gyrations...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_16_0</td>\n",
       "      <td>However, the lower prices these retail chains ...</td>\n",
       "      <td>However, &lt;ARG0&gt;the lower prices these retail c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_18_0</td>\n",
       "      <td>\"What's encouraging about this is that retail ...</td>\n",
       "      <td>\"What's encouraging about this is that retail ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_19_0</td>\n",
       "      <td>Still, most retailing observers expect that al...</td>\n",
       "      <td>Still, most retailing observers expect that al...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>because</td>\n",
       "      <td>wsj_2125.ann</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>because_wsj_2125.ann_20_0</td>\n",
       "      <td>\"Prices for retail chains are lower today than...</td>\n",
       "      <td>\"Prices for retail chains are lower today than...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1245 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     corpus                   doc_id sent_id eg_id  \\\n",
       "0   because  CHRG-110hhrg44900-1.ann       5     0   \n",
       "1   because  CHRG-110hhrg44900-1.ann      13     0   \n",
       "2   because  CHRG-110hhrg44900-1.ann      15     0   \n",
       "3   because  CHRG-110hhrg44900-1.ann      25     0   \n",
       "4   because  CHRG-110hhrg44900-1.ann      38     0   \n",
       "..      ...                      ...     ...   ...   \n",
       "7   because             wsj_2125.ann      14     0   \n",
       "8   because             wsj_2125.ann      16     0   \n",
       "9   because             wsj_2125.ann      18     0   \n",
       "10  because             wsj_2125.ann      19     0   \n",
       "11  because             wsj_2125.ann      20     0   \n",
       "\n",
       "                                   index  \\\n",
       "0    because_CHRG-110hhrg44900-1.ann_5_0   \n",
       "1   because_CHRG-110hhrg44900-1.ann_13_0   \n",
       "2   because_CHRG-110hhrg44900-1.ann_15_0   \n",
       "3   because_CHRG-110hhrg44900-1.ann_25_0   \n",
       "4   because_CHRG-110hhrg44900-1.ann_38_0   \n",
       "..                                   ...   \n",
       "7              because_wsj_2125.ann_14_0   \n",
       "8              because_wsj_2125.ann_16_0   \n",
       "9              because_wsj_2125.ann_18_0   \n",
       "10             because_wsj_2125.ann_19_0   \n",
       "11             because_wsj_2125.ann_20_0   \n",
       "\n",
       "                                                 text  \\\n",
       "0   We have gotten the agreement of the Chairman a...   \n",
       "1   Members want to get a new context because the ...   \n",
       "2   Given the importance of this, and given the in...   \n",
       "3   Where there was a strong argument as recently ...   \n",
       "4   Something that simple causes problems in subpr...   \n",
       "..                                                ...   \n",
       "7   Not only could the Wall Street gyrations damp ...   \n",
       "8   However, the lower prices these retail chains ...   \n",
       "9   \"What's encouraging about this is that retail ...   \n",
       "10  Still, most retailing observers expect that al...   \n",
       "11  \"Prices for retail chains are lower today than...   \n",
       "\n",
       "                                         text_w_pairs  seq_label  pair_label  \\\n",
       "0   We have gotten the agreement of the Chairman a...          1           1   \n",
       "1   <ARG1>Members want to get a new context</ARG1>...          1           1   \n",
       "2   Given <ARG0>the importance of this</ARG0>, and...          1           1   \n",
       "3   Where there was a strong argument as recently ...          1           1   \n",
       "4   Something that simple causes problems in subpr...          1           1   \n",
       "..                                                ...        ...         ...   \n",
       "7   Not only <ARG1>could the Wall Street gyrations...          1           1   \n",
       "8   However, <ARG0>the lower prices these retail c...          1           1   \n",
       "9   \"What's encouraging about this is that retail ...          0           0   \n",
       "10  Still, most retailing observers expect that al...          1           1   \n",
       "11  \"Prices for retail chains are lower today than...          1           1   \n",
       "\n",
       "   context  num_sents  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "..     ...        ...  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  \n",
       "10                  1  \n",
       "11                  1  \n",
       "\n",
       "[1245 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for counter, fn in enumerate(list_of_filepaths):\n",
    "    \n",
    "    if 'NYT' in fn:\n",
    "        # we do not have access to these\n",
    "        continue\n",
    "        \n",
    "    df = run(fn)\n",
    "    data = pd.concat([data, df])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11cd509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have gotten the agreement of the Chairman and the Secretary, preliminary to any opening statements, to stay until 1 p.m. We will probably have some votes, so we will maximize our time.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['text'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ca35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Examples\n",
      "Seq Level: Counter({1: 1039, 0: 206})\n",
      "Pair Level: Counter({1: 965, 0: 280})\n",
      "Seq Level (Unique): Counter({1: 761, 0: 193})\n",
      "\n",
      "Single Sentence Examples\n",
      "Seq Level: Counter({1: 1039, 0: 206})\n",
      "Pair Level: Counter({1: 965, 0: 280})\n",
      "Seq Level (Unique): Counter({1: 761, 0: 193})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('All Examples')\n",
    "print('Seq Level:', Counter(data['seq_label'])) # if sentence level causality exists\n",
    "print('Pair Level:',Counter(data['pair_label'])) # if ARG0-ARG1 pair level causality exists\n",
    "print('Seq Level (Unique):',Counter(data.drop_duplicates(subset=['corpus','doc_id','sent_id'])['seq_label']))\n",
    "\n",
    "print('\\nSingle Sentence Examples')\n",
    "print('Seq Level:', Counter(data.loc[data['num_sents']==1,'seq_label'])) # if sentence level causality exists\n",
    "print('Pair Level:', Counter(data.loc[data['num_sents']==1,'pair_label'])) # if ARG0-ARG1 pair level causality exists\n",
    "print('Seq Level (Unique):',Counter(data.loc[data['num_sents']==1].drop_duplicates(subset=['corpus','doc_id','sent_id'])['seq_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2ab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned/because.csv', index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a03589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

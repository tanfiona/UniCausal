{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce our unified model to handle the three tasks: sequence classification, span detection, and pair classification. Full model structure is clearly depicted in the below [figure](../assets/unicausal_model.png). \n",
    "\n",
    "### Model Structure\n",
    "\n",
    "It adopts a BERT embedding layer as a backbone to transform input texts into sequence embeddings. We may use [predefined HuggingFace model configuration](https://huggingface.co/docs/transformers/model_doc/bert) to initialize the BERT layer. It then feeds the token embeddings into a couple of downstream span token classifiers - the exact number depends on how many spans are there in a sequence. Although named span detection, it is actually a classification task as we are assigning a range of labels to input tokens.\n",
    "\n",
    "Each span classifier is connected in the sense that prior classifier will feed its output logits to the next one, along with the tokenized sequence. This makes sense considering pairs of cause and effect spans usually have dependence on each other. \n",
    "\n",
    "For sequence and pair classification, it requires a more global view of the input sequence, thus the tokens will go through an extra step of pooling layer, before getting fed into separate sequence and pair classifers. The cross-entropy losses from above mentioned structures are aggregated using weighted summation, where a tunable scaling factor $\\alpha$ is used to weigh the importance of token and sequence classification loss. \n",
    "\n",
    "These implementation details can be found in our [paper]() or in the later model signature section.\n",
    "\n",
    "### Load the Model\n",
    "\n",
    "Now we will show how to load the unified model for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from ..models.classifiers.modeling_bert import BertForUnifiedCRBase\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    ") # other imports are omitted for clarification\n",
    "\n",
    "# Instantiate config\n",
    "# either provide a pretrained config name or model path, or a model type to initialize a config.\n",
    "# for tutorial of transformers config, see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "config_name = model_name_or_path = None\n",
    "model_type = 'roberta'\n",
    "# Example token labels to ints mapping: {'B-C': 0, 'B-E': 1, 'I-C': 2, 'I-E': 3, 'O': 4}\n",
    "# for the meaning of each label, see our paper: \n",
    "num_token_labels = 5\n",
    "\n",
    "if config_name:\n",
    "    config = AutoConfig.from_pretrained(config_name, num_labels=num_token_labels)\n",
    "elif model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_token_labels)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[model_type]()\n",
    "\n",
    "# load model from config\n",
    "if model_name_or_path:\n",
    "    model = BertForUnifiedCRBase.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "        num_seq_labels=2,\n",
    "        loss_function='simple',\n",
    "        alpha=3 # parameter mentioned before, for weighing token and sequence loss significance\n",
    "    )\n",
    "else:\n",
    "    print(\"Training new model from scratch\")\n",
    "    model = BertForUnifiedCRBase.from_config(config)\n",
    "# Now the model is ready for use. If you would like to see the full initialization process, check out \n",
    "# our training script at `../run.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is even more illuminating to see how the model works internally as a HuggingFace [`BertPreTrainedModel`](https://huggingface.co/docs/transformers/main_classes/model) subclass. We have attached all relevant BertForUnifiedCRBase model function signatures (simplified) and i/o below for illustration.\n",
    "\n",
    "```\n",
    "class Pooler(nn.Module):\n",
    "    \"\"\"Pooler model to condense input sequence embeddings\"\"\"\n",
    "    def __init__(self, seq_length, condensed_size):\n",
    "        ...\n",
    "\n",
    "class BertForUnifiedCR(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, ...):\n",
    "        super().__init__(config)\n",
    "        ...\n",
    "        self.bert = BertModel(config, ...)\n",
    "        self.dropout = nn.Dropout(...)\n",
    "        ...\n",
    "        \n",
    "        self.reader = nn.LSTM(...)\n",
    "        self.linear = nn.Linear(...)\n",
    "        self.linear2 = nn.Linear(...) \n",
    "        self.tokclf = nn.Linear(...) # token classification layer\n",
    "        self.pool = Pooler(...)\n",
    "        self.seqclf = nn.Linear(...) # sequence classification layer\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        ... # other inputs to bert model\n",
    "    ):\n",
    "\n",
    "        # compute bert embeddings\n",
    "        outputs = self.bert(\n",
    "            input_ids=...,\n",
    "            attention_mask=...,\n",
    "            ...\n",
    "        )\n",
    "        \n",
    "        # take first BERT token embedding as sequence classifier input\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = some_function_to_add_other_features(sequence_output, ...)\n",
    "        \n",
    "        # lstm processing\n",
    "        sequence_output = self.reader(some_processing_function(sequence_output)) # recall what self.reader was\n",
    "        \n",
    "        # condense\n",
    "        sequence_output = self.linear(some_other_processing_function(sequence_output))\n",
    "        sequence_output = self.linear2(some_other_processing_function(sequence_output))\n",
    "                               \n",
    "        # perform token classification\n",
    "        sequence_output = self.dropout(sequence_output) # either toggled on or off\n",
    "        tok_logits = self.tokclf(sequence_output)\n",
    "        tok_loss = some_function_to_compute_loss(tok_logits, ...)\n",
    "\n",
    "        # sequence/pair classification\n",
    "        pooled_output=self.pool(tok_logits) # pooling operation for sequence inputs\n",
    "        logits = self.seqclf(pooled_output)\n",
    "        loss = some_other_function_to_compute_loss(logits, ...)\n",
    "        \n",
    "        # return predictions and losses\n",
    "        return (loss, logits, tok_loss, tok_logits,)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,...):\n",
    "        \"\"\"Used when pretrained weights available. Will elaborate in the next notebook.\"\"\"\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with HF Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder how our unified model structure compares to HuggingFace off-the-shelf classifiers for the three tasks. Below is an equivalent model instantiation script for a HuggingFace [pre-trained model](https://huggingface.co/docs/transformers/model_doc/auto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config instantiation process is the same as above\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "if model_name_or_path:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForTokenClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleared doubts about how to load in our unified model. It is time to train it up. In the next tutorial, we will introduce how to make predictions using our pretrained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this particular tutorial, we explain the three types of tasks and their required datasets. We cover how to load our prepared datasets or load your very own datasets using our provided functionalities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [The Tasks](#task)\n",
    "- [Dataset Types](#type)\n",
    "- [Dataset Sources](#source)\n",
    "- [Loading the Datasets](#load)\n",
    "    - [Preview of examples from our combined dataset](#preview)\n",
    "    - [Using your own dataset](#own)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tasks\n",
    "<a id='task'></a>\n",
    "\n",
    "The three tasks, as shown in [Figure 1](../assets/tasks.png), are Sequence Classification, Span Detection, and Pair Classification. By definition:\n",
    "1. Sequence Classification: Given an example sequence, do it contain causal relationships?\n",
    "2. Span Detection: Given a causal sequence example, which words in the sentence correspond to\n",
    "the Cause and Effect arguments? The task is to identify up to three causal relations and their spans.\n",
    "3. Pair Classification: Given sentences with marked argument or entity pairs, the task is to figure out if they are causally related, such that the first argument (marked as `ARG0`) causes the second argument (`ARG1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types\n",
    "<a id='type'></a>\n",
    "\n",
    "Correspondingly, there are three type of datasets needed for training purposes, abbreviated as `Seq` type, `Span` type, as well as `Pair` type. \n",
    "1. `Seq` type datasets contain both causal and non-causal texts, where each unique example text is labelled with a target `s`. Causal texts refer to texts that contain causal relationships. \n",
    "2. `Span` type datasets contain only causal texts. Each unique example text allows up to three causal relations. To annotate the text, we converted spans into a BIO-format (Begin (B), Inside (I), Outside (O))  for two types of spans (Cause (C), Effect (E)). Therefore, there were five possible labels per word: B-C, I-C,\n",
    "B-E, I-E and O, and the task is to predice the labels for each word. For examples with multiple relations, we sorted them based on the location of the B-C, followed by B-E if tied. This means that an earlier occurring Cause span was assigned a lower index number. See Figure 1â€™s spans for example.\n",
    "3. `Pair` type datasets contain both causal and non-causal texts. Special tokens (`<ARG0>`, `</ARG0>`) marks the boundaries of a Cause span, while (`<ARG1>`, `</ARG1>`) marks the boundaries of a corresponding Effect span. Each example text may contain multiple pairs of arguments, resulting in differently located argument tokens `ARG0` and `ARG1`. For a given text of length `N`, say it has `a` number of arguments, the input word vector $\\vec u$ has length `N+2*a` due to the addition of special tokens. Finally tokenized sequence $\\vec w$ can have multiple versions of $\\vec u$ due to differently located argument tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Sources\n",
    "<a id='source'></a>\n",
    "\n",
    "We have processed and split 6 corpus ([AltLex](https://aclanthology.org/P16-1135/), [BECAUSE](https://aclanthology.org/W17-0812/), [CTB](https://aclanthology.org/W14-0702/), [ESL](https://aclanthology.org/W17-2711/), [PDTB](https://catalog.ldc.upenn.edu/LDC2019T05), [Sem-Eval](https://aclanthology.org/S10-1006)) into the specified three types of datasets for your convenient use. The statistics are as below.<br>\n",
    "<img src=\"../assets/temp_statistics.png\" alt=\"Table\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "<a id='load'></a>\n",
    "\n",
    "To load the datasets, we have provided convenient interfaces. In the [training script](../run.sh), add `--dataset_name` attribute and append the dataset names you want. For example, `--dataset_name altlex because` means to load and train the model on [AltLex](https://aclanthology.org/P16-1135/), [BECAUSE](https://aclanthology.org/W17-0812/) datasets. Full list of provided datasets are <code>['altlex', 'because', 'ctb', 'esl', 'esl2', 'pdtb', 'semeval2010t8', 'cnc', 'causenet', 'causenetm']</code>.\n",
    "\n",
    "In case you want to use our `load_cre_dataset` function to load the datasets manually. The function signature is defined as:\n",
    "```\n",
    "def load_cre_dataset(\n",
    "        dataset_name: List[str],\n",
    "        do_train_val: bool,\n",
    "        also_add_span_sequence_into_seq: bool = False, \n",
    "        span_augment: bool = False,\n",
    "        span_files: dict = {}, \n",
    "        seq_files: dict = {}, \n",
    "        do_train: bool = True) -> Tuple[DatasetDict, DatasetDict, Tuple[int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Loads in specified dataset from pre-processed training and testing files, or user-provided span \n",
    "    and seq files. \n",
    "\n",
    "    Args:\n",
    "        dataset_name: A list of dataset names intend to be loaded\n",
    "        do_train_val: A boolean value indicating whether to load validation datasets\n",
    "        also_add_span_sequence_into_seq: A boolean value indicating whether to add span texts to sequence texts\n",
    "        span_augment: A boolean value indicating whether to retain only the Cause or Effect clause as a Non-causal example to augment the span dataset\n",
    "        span_files: A dictionary of user provided span data files, in the format of \n",
    "{'train':path_to_training_files, 'valid': path_to_valid_files}\n",
    "        seq_files: A dictionary of user provided sequence data files, in the format of \n",
    "{'train': path_to_training_files, 'valid': path_to_valid_files}\n",
    "        do_train: A boolean value indicating whether to do the training process\n",
    "    Returns: A ``Tuple`` of interest\n",
    "    Raises:\n",
    "        ValueError: Raises an ValueError if input dataset_name doesn't exist, or provided seq files have 0 or more than 3 or causal relations per text.\n",
    "    \"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "See more details in the code example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available datasets: ['altlex', 'because', 'ctb', 'esl', 'esl2', 'pdtb', 'semeval2010t8', 'cnc', 'causenet', 'causenetm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-196cccd548475efd\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-196cccd548475efd/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ca9bc3e9a94cb991ea7e7381574949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     span_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 127\n",
       "     })\n",
       "     span_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 606\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     seq_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 290\n",
       "     })\n",
       "     pair_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 435\n",
       "     })\n",
       "     seq_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 374\n",
       "     })\n",
       "     pair_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 1178\n",
       "     })\n",
       " }),\n",
       " (5, 1, 3, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from _datasets.unifiedcre import load_cre_dataset, available_datasets\n",
    "print('List of available datasets:', available_datasets)\n",
    "\n",
    "\"\"\"\n",
    " Example case of loading AltLex and BECAUSE dataset,\n",
    " without adding span texts to seq texts, span augmentation or user-provided datasets,\n",
    " and load both training and validation datasets.\n",
    "\"\"\"\n",
    "load_cre_dataset(dataset_name=['altlex','because'], do_train_val=True, data_dir='../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have experience working with the [`load_dataset`](https://huggingface.co/docs/datasets/loading) function the HuggingFace datasets library. Our method can be taken as a wrapper function of HuggingFace [`load_dataset`](https://huggingface.co/docs/datasets/loading), which loads three types of datasets simultaneously and applies some customized loading steps to the datasets, as datasets such as of `Span` type need to loaded with special care to handle their labels. Note that they have different function signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of examples from our combined dataset\n",
    "<a id='preview'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sources_to_show = ['altlex', 'because', 'ctb', 'esl', 'pdtb', 'semeval2010t8'] # esl and pdtb not available\n",
    "dataset = load_cre_dataset(\n",
    "    dataset_name=dataset_sources_to_show, \n",
    "    do_train_val=True, \n",
    "    data_dir='../data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': 'altlex', 'index': 'altlex_altlex_dev.tsv_4_0', 'text': ['The', 'U.S.', 'Supreme', 'Court', 'refused', 'to', 'hear', 'an', 'appeal', 'of', 'the', 'decision', 'of', 'the', 'lower', 'federal', 'courts', 'in', 'October', '1993', ',', 'meaning', 'that', 'victims', 'of', 'the', 'Bhopal', 'disaster', 'could', 'not', 'seek', 'damages', 'in', 'a', 'U.S.', 'court', '.'], 'label': 1, 'ce_tags': ['B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E'], 'ce_tags1': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ce_tags2': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']} \n",
      "\n",
      "{'corpus': 'because', 'index': 'because_Article247_327.ann_3_1', 'text': ['They', 'will', 'then', 'score', 'one', 'point', 'for', 'every', 'subsequent', 'issue', 'or', 'broadcast', 'or', 'Internet', 'posting', 'after', 'the', 'first', 'offense', 'is', 'noted', 'by', 'Chatterbox', 'if', 'they', 'continue', 'not', 'to', 'report', 'said', 'inconvenient', 'fact--and', 'an', 'additional', 'two', 'points', 'on', 'days', 'when', 'the', 'news', 'organization', 'runs', 'a', 'follow-up', 'without', 'making', 'note', 'of', 'said', 'inconvenient', 'fact.'], 'label': 1, 'ce_tags': ['B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ce_tags1': ['B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ce_tags2': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Span examples\n",
    "dataset_sources_shown = []\n",
    "for i in dataset[0]['span_validation']:\n",
    "    corpus = i['corpus']\n",
    "    if corpus not in dataset_sources_shown:\n",
    "        print(i,'\\n')\n",
    "        dataset_sources_shown.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': 'altlex', 'index': 'altlex_altlex_dev.tsv_0_0', 'text': \"The Bhopal disaster , also referred to as the Bhopal gas tragedy , was a gas leak incident in India , considered the world 's worst industrial disaster .\", 'label': 0} \n",
      "\n",
      "{'corpus': 'because', 'index': 'because_Article247_327.ann_18_0', 'text': \"But the Scientific Method does not permit any tinkering with the Indis Index 's scoring procedures.\", 'label': 0} \n",
      "\n",
      "{'corpus': 'ctb', 'index': 'ctb_APW19980213.1320.tml_0_0', 'text': 'CANBERRA, Australia ( AP ) _ Qantas will almost double its flights between Australia and India by August in the search for new markets untouched by the crippling Asian financial crisis.', 'label': 0} \n",
      "\n",
      "{'corpus': 'semeval2010t8', 'index': 'semeval2010t8_test.json_0_0', 'text': 'The most common audits were about waste and recycling .', 'label': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seq examples\n",
    "dataset_sources_shown = []\n",
    "for i in dataset[1]['seq_validation']:\n",
    "    corpus = i['corpus']\n",
    "    if corpus not in dataset_sources_shown:\n",
    "        print(i,'\\n')\n",
    "        dataset_sources_shown.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': 'altlex', 'index': 'altlex_altlex_dev.tsv_0_0', 'text': \"<ARG1>The Bhopal disaster , also referred to</ARG1> as <ARG0>the Bhopal gas tragedy , was a gas leak incident in India , considered the world 's worst industrial disaster .</ARG0>\", 'label': 0} \n",
      "\n",
      "{'corpus': 'because', 'index': 'because_Article247_327.ann_3_0', 'text': 'They will then score one point for <ARG1>every subsequent issue or broadcast or Internet posting</ARG1> after <ARG0>the first offense is noted by Chatterbox</ARG0> if they continue not to report said inconvenient fact--and an additional two points on days when the news organization runs a follow-up without making note of said inconvenient fact.', 'label': 0} \n",
      "\n",
      "{'corpus': 'ctb', 'index': 'ctb_APW19980213.1320.tml_0_0', 'text': 'CANBERRA, Australia ( AP ) _ Qantas will almost <ARG0>double</ARG0> its flights between Australia and India by <ARG1>August</ARG1> in the search for new markets untouched by the crippling Asian financial crisis.', 'label': 0} \n",
      "\n",
      "{'corpus': 'semeval2010t8', 'index': 'semeval2010t8_test.json_0_0', 'text': 'The most common <ARG0>audits</ARG0> were about <ARG1>waste</ARG1> and recycling .', 'label': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seq examples\n",
    "dataset_sources_shown = []\n",
    "for i in dataset[1]['pair_validation']:\n",
    "    corpus = i['corpus']\n",
    "    if corpus not in dataset_sources_shown:\n",
    "        print(i,'\\n')\n",
    "        dataset_sources_shown.append(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your own dataset\n",
    "<a id='own'></a>\n",
    "\n",
    "In certain scenarios, you may want to use your own datasets to test the power of our unifed task training. Fortunately, our dataset loaders are open to user provided training and testing files. When using our training script, it is as easy as appending your training and validation file paths to arguments `--span_train_file`, `--span_val_file`, `--seq_train_file`, and `--seq_val_file`, with each path leading to a `csv`, `json`, or `txt` file that contains the corresponding dataset. \n",
    "\n",
    "They can be the paths to your very own datasets, or the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/. The column name of text and labels (for `csv` or `json` files) can be set via arguments `--text_column_name` and `--label_column_name`.\n",
    "\n",
    "Our `run.py` script will automatically process the input paths and handle the rest of job before model training.\n",
    "\n",
    "If you wish to load your own datasets manually using our `load_cre_dataset` function, follow the steps below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-4926b80379fbba28\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-4926b80379fbba28/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dda0fd90224532818e7cbc6deef382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-33ea68081fc94ed9\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-33ea68081fc94ed9/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3fe07bd7e0467083069af255008894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     span_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 115\n",
       "     })\n",
       "     span_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 300\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     seq_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 687\n",
       "     })\n",
       "     pair_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 832\n",
       "     })\n",
       "     seq_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 854\n",
       "     })\n",
       "     pair_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 1222\n",
       "     })\n",
       " }),\n",
       " (7, 1, 4, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input arguments\n",
    "dataset_name = None # required when using customsized datasets\n",
    "span_augment = False\n",
    "do_train = do_eval = do_predict = do_train_val = True\n",
    "\n",
    "# Using your own files\n",
    "span_train_file = 'data/grouped/splits/altlex_train.csv'\n",
    "span_val_file = 'data/grouped/splits/altlex_test.csv'\n",
    "seq_train_file = 'data/splits/altlex_train.csv'\n",
    "seq_val_file = 'data/splits/altlex_test.csv'\n",
    "\n",
    "# [Not sure if supported] Using huggingface datasets (https://huggingface.co/datasets)\n",
    "# dataset_name = ['wikitext']\n",
    "# xxx\n",
    "\n",
    "# Load file paths into dictionaries\n",
    "span_files, seq_files = {}, {}\n",
    "span_files[\"train\"] = span_train_file\n",
    "span_files[\"validation\"] = span_val_file\n",
    "seq_files[\"train\"] = seq_train_file\n",
    "seq_files[\"validation\"] = seq_val_file\n",
    "\n",
    "# Call load_cre_dataset function\n",
    "load_cre_dataset(\n",
    "    dataset_name, do_train_val,\n",
    "    span_augment=span_augment,\n",
    "    span_files=span_files, \n",
    "    seq_files=seq_files,\n",
    "    do_train=do_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It comes to the end of our dataset usage tutorial. We are now ready to start model loading section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

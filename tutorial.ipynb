{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular tutorial, we explain the three types of tasks and their required datasets. We cover how to load our prepared datasets or load your very own datasets using our provided functionalities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three tasks, as shown in Figure [1](assets/tasks.png), are Sequence Classification, Span Detection, and Pair Classification. By definition:\n",
    "1. Sequence Classification: Given an example sequence, do it contain causal relationships?\n",
    "2. Span Detection: Given a causal sequence example, which words in the sentence correspond to\n",
    "the Cause and Effect arguments? The task is to identify up to three causal relations and their spans.\n",
    "3. Pair Classification: Given sentences with marked argument or entity pairs, the task is to figure out if they are causally related, such that the first argument (marked as `ARG0`) causes the second argument (`ARG1`).\n",
    "\n",
    "Correspondingly, there are three type of datasets needed for training purposes, abbreviated as `Seq` type, `Span` type, as well as `Pair` type. \n",
    "1. `Seq` type datasets contain both causal and non-causal texts, where each unique example text is labelled with a target `s`. Causal texts refer to texts that contain causal relationships. \n",
    "2. `Span` type datasets contain only causal texts. Each unique example text allows up to three causal relations. To annotate the text, we converted spans into a BIO-format (Begin (B), Inside (I), Outside (O))  for two types of spans (Cause (C), Effect (E)). Therefore, there were five possible labels per word: B-C, I-C,\n",
    "B-E, I-E and O, and the task is to predice the labels for each word. For examples with multiple relations, we sorted them based on the location of the B-C, followed by B-E if tied. This means that an earlier occurring Cause span was assigned a lower index number. See Figure 1â€™s spans for example.\n",
    "3. `Pair` type datasets contain both causal and non-causal texts. Special tokens (`<ARG0>`, `</ARG0>`) marks the boundaries of a Cause span, while (`<ARG1>`, `</ARG1>`) marks the boundaries of a corresponding Effect span. Each example text may contain multiple pairs of arguments, resulting in differently located argument tokens `ARG0` and `ARG1`. For a given text of length `N`, say it has `a` number of arguments, the input word vector $\\vec u$ has length `N+2*a` due to the addition of special tokens. Finally tokenized sequence $\\vec w$ can have multiple versions of $\\vec u$ due to differently located argument tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed and split 6 corpus ([AltLex](), [BECAUSE](), [CTB](), [ESL](), [PDTB](), [Sem-Eval]()) into the specified three types of datasets for your convenient use. The statistics are as below.<br>\n",
    "<img src=\"assets/temp_statistics.png\" alt=\"Table\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the datasets, we have provided convenient interfaces. In the [training script](_run.sh), add `--dataset_name` attribute and append the dataset names you want. For example, `--dataset_name altlex because` means to load and train the model on [AltLex]() and [BECAUSE]() datasets. Full list of provided datasets are <code>['altlex', 'because', 'ctb', 'esl', 'esl2', 'pdtb', 'semeval2010t8', 'cnc', 'causenet', 'causenetm']</code>.\n",
    "\n",
    "In case you want to use our `load_cre_dataset` function to load the datasets manually. The function signature is defined as:\n",
    "```\n",
    "def load_cre_dataset(\n",
    "        dataset_name: List[str],\n",
    "        do_train_val: bool,\n",
    "        also_add_span_sequence_into_seq: bool = False, \n",
    "        span_augment: bool = False,\n",
    "        span_files: dict = {}, \n",
    "        seq_files: dict = {}, \n",
    "        do_train: bool = True) -> Tuple[DatasetDict, DatasetDict, Tuple[int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Loads in specified dataset from pre-processed training and testing files, or user-provided span \n",
    "    and seq files. \n",
    "\n",
    "    Args:\n",
    "        dataset_name: A list of dataset names intend to be loaded\n",
    "        do_train_val: A boolean value indicating whether to load validation datasets\n",
    "        also_add_span_sequence_into_seq: A boolean value indicating whether to add span texts \n",
    "        to sequence texts\n",
    "        span_augment: A boolean value indicating whether to retain only the Cause or Effect clause as a Non-\n",
    "        causal example to augment the span dataset\n",
    "        span_files: A dictionary of user provided span data files, in the format of {'train': path_to_training_files, 'valid': path_to_valid_files}\n",
    "        seq_files: A dictionary of user provided sequence data files, in the format of {'train': path_to_training_files, 'valid': path_to_valid_files}\n",
    "        do_train: A boolean value indicating whether to do the training process\n",
    "    Returns: A ``Tuple`` of interest\n",
    "    Raises:\n",
    "        ValueError: Raises an ValueError if input dataset_name doesn't exist, or provided seq files \n",
    "        have 0 or more than 3 or causal relations per text.\n",
    "    \"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "See more details in the code example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available datasets: ['altlex', 'because', 'ctb', 'esl', 'esl2', 'pdtb', 'semeval2010t8', 'cnc', 'causenet', 'causenetm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-196cccd548475efd\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-196cccd548475efd/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ef29bce2994374b4fa8cd6656ce10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     span_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 127\n",
       "     })\n",
       "     span_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 606\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     seq_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 290\n",
       "     })\n",
       "     pair_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 435\n",
       "     })\n",
       "     seq_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 374\n",
       "     })\n",
       "     pair_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 1178\n",
       "     })\n",
       " }),\n",
       " (5, 1, 3, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from _datasets.unifiedcre import load_cre_dataset, available_datasets\n",
    "\n",
    "print('List of available datasets:', available_datasets)\n",
    "\n",
    "\"\"\"\n",
    " Example case of loading altlex and because dataset,\n",
    " without adding span texts to seq texts, span augmentation or user-provided datasets,\n",
    " and load both training and validation datasets.\n",
    "\"\"\"\n",
    "load_cre_dataset(dataset_name=['altlex','because'], do_train_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have experience working with the [`load_dataset`](https://huggingface.co/docs/datasets/loading) function the HuggingFace datasets library. Our method can be taken as a wrapper function of HuggingFace [`load_dataset`](https://huggingface.co/docs/datasets/loading), which loads three types of datasets simultaneously and applies some customized loading steps to the datasets, as datasets such as of `Span` type need to loaded with special care to handle their labels. Note that they have different function signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain scenarios, you may want to use your own datasets to test the power of our unifed task training. Fortunately, our dataset loaders are open to user provided training and testing files. When using our training script, it is as easy as appending your training and validation file paths to arguments `--span_train_file`, `--span_val_file`, `--seq_train_file`, and `--seq_val_file`, with each path leading to a `csv`, `json`, or `txt` file that contains the corresponding dataset. \n",
    "\n",
    "They can be the paths to your very own datasets, or the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/. The column name of text and labels (for `csv` or `json` files) can be set via arguments `--text_column_name` and `--label_column_name`.\n",
    "\n",
    "Our `run.py` script will automatically process the input paths and handle the rest of job before model training.\n",
    "\n",
    "If you wish to load your own datasets manually using our `load_cre_dataset` function, follow the steps below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-4926b80379fbba28\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-4926b80379fbba28/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dda0fd90224532818e7cbc6deef382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-33ea68081fc94ed9\n",
      "WARNING:datasets.builder:Reusing dataset csv (/home/jiatong/.cache/huggingface/datasets/csv/default-33ea68081fc94ed9/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3fe07bd7e0467083069af255008894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     span_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 115\n",
       "     })\n",
       "     span_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label', 'ce_tags', 'ce_tags1', 'ce_tags2'],\n",
       "         num_rows: 300\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     seq_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 687\n",
       "     })\n",
       "     pair_validation: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 832\n",
       "     })\n",
       "     seq_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 854\n",
       "     })\n",
       "     pair_train: Dataset({\n",
       "         features: ['corpus', 'index', 'text', 'label'],\n",
       "         num_rows: 1222\n",
       "     })\n",
       " }),\n",
       " (7, 1, 4, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input arguments\n",
    "dataset_name = None # required when using customsized datasets\n",
    "span_augment = False\n",
    "do_train = do_eval = do_predict = do_train_val = True\n",
    "\n",
    "# Using your own files\n",
    "span_train_file = 'data/grouped/splits/altlex_train.csv'\n",
    "span_val_file = 'data/grouped/splits/altlex_test.csv'\n",
    "seq_train_file = 'data/splits/altlex_train.csv'\n",
    "seq_val_file = 'data/splits/altlex_test.csv'\n",
    "\n",
    "# [Not sure if supported] Using huggingface datasets (https://huggingface.co/datasets)\n",
    "# dataset_name = ['wikitext']\n",
    "# xxx\n",
    "\n",
    "# Load file paths into dictionaries\n",
    "span_files, seq_files = {}, {}\n",
    "span_files[\"train\"] = span_train_file\n",
    "span_files[\"validation\"] = span_val_file\n",
    "seq_files[\"train\"] = seq_train_file\n",
    "seq_files[\"validation\"] = seq_val_file\n",
    "\n",
    "# Call load_cre_dataset function\n",
    "load_cre_dataset(\n",
    "    dataset_name, do_train_val,\n",
    "    span_augment=span_augment,\n",
    "    span_files=span_files, \n",
    "    seq_files=seq_files,\n",
    "    do_train=do_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It comes to the end of our dataset usage tutorial. We are now ready to start model loading section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
